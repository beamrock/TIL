[Amazon S3 및 AWS Glue를 이용한 데이터 레이크 구축하기](https://aws.amazon.com/ko/blogs/korea/build-a-data-lake-foundation-with-aws-glue-and-amazon/?utm_source=dlvr.it&utm_medium=facebook) 라는 AWS 블로그 글을 훑어봤다. `Data Lake` 라는건 서비스 이름이 아니라 개념이었다. 뒷단의 여러 주체들이 사용할 수 있도록 structured / unstructured 관계없이 raw 데이터를 쌓아놓는 장소를 의미하며, AWS 서비스들 중에는 S3 가 적절하다. 이때 **Glue** 가 등장하는 배경은 Data Lake 에 쌓인 raw data 를 뒷단의 데이터 분석 주체들이 사용할 수 있는 형태로 가공하는 ETL 작업이다. 예를 들어 CSV 포맷의 raw data 가 S3 에 저장되면, AWS Athena 에서 효율적으로 분석할 수 있는 Parquet 포맷으로 변환하는 작업을 Glue 가 담당한다. 이 작업은 AWS Lambda 를 활용해서 S3 에 새로운 object 가 추가되는 시점에 trigger 되게 설정할 수도 있을 것이다. Glue 의 유용한 점은 S3 저장소 경로만 지정하면, **데이터를 알아서 크롤링해서 schema 를 파악하고, CSV 에서 Parquet 포맷으로 변환하는 류의 보편적인 ETL 작업에 대해서는 script 도 자동으로 생성해준다.** 그러므로 방금 언급한 보편적인 ETL 작업들은 데이터 분석을 위한 코드를 작성할 필요 없이 AWS Console 에서 GUI 로 셋업할 수 있는 것이다. 데이터 엔지니어로서는 잘 활용해야할 대상이며, 또한 내 직무가 클라우드 서비스에 의해 대체될 수 있다는 점을 시사하는 아찔한 존재다.  